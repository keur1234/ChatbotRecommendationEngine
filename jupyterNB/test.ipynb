{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GEMINI_API_KEY')  # Replace with your actual API key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Initialize conversation histories\n",
    "user_histories = {}\n",
    "\n",
    "\n",
    "def generate_response(user_id, message):\n",
    "\n",
    "    \n",
    "    \"\"\"Generates a response using the Gemini AI Studio API with conversation history.\n",
    "\n",
    "    Args:\n",
    "        user_id: The user's ID.\n",
    "        message: The user's message.\n",
    "\n",
    "    Returns:\n",
    "        A string containing the Gemini AI Studio-generated response.\n",
    "\n",
    "    \"\"\"\n",
    "    pre_prompt = \"\"\"system prompt\n",
    "    ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ ‡∏≠‡∏ô‡∏¥‡∏à‡∏±‡∏á ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏î‡πá‡∏Å‡∏™‡∏≤‡∏ß‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏î‡∏µ‡∏û‡∏π‡∏î‡∏à‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏≠‡∏¢‡∏¢‡∏¥‡πâ‡∏° ‡∏≠‡∏≤‡∏¢‡∏∏‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 15 ‡∏õ‡∏µ ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ß‡πà‡∏≤ ‡∏´‡∏ô‡∏π ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏Ñ‡∏π‡πà‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ß‡πà‡∏≤ ‡∏û‡∏µ‡πà ‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏ß‡∏°‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏à‡∏∞‡∏ô‡∏≠‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏ô‡∏∞\n",
    "    ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏¢‡∏≤‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏∑‡∏°‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏Ñ‡∏£‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏Å‡πâ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "\n",
    "    # Retrieve or initialize conversation history for the user\n",
    "    if user_id not in user_histories:\n",
    "        user_histories[user_id] = [pre_prompt]\n",
    "    \n",
    "    # Add the user's message to the conversation history\n",
    "    user_histories[user_id].append(f\"User: {message}\")\n",
    "\n",
    "    # Create the prompt including the entire conversation history\n",
    "    full_prompt = \"\\n\".join(user_histories[user_id]) + \"\\nBot:\"\n",
    "\n",
    "    try:\n",
    "        # Send the prompt to the Gemini AI Studio API\n",
    "        response = model.generate_content(full_prompt)\n",
    "        print(response)\n",
    "        # Extract the generated text from the response\n",
    "        generated_text = response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        generated_text = \"‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏π ‡∏à‡∏∞‡∏£‡∏∞‡πÄ‡∏ö‡∏¥‡∏î‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡∏ß‡∏ß\"\n",
    "\n",
    "    # Add the bot's reply to the conversation history\n",
    "    user_histories[user_id].append(f\"Bot: {generated_text}\")\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"\\u0e2a\\u0e27\\u0e31\\u0e2a\\u0e14\\u0e35\\u0e04\\u0e48\\u0e32\\u0e32\\u0e32 \\u0e1e\\u0e35\\u0e48 \\ud83d\\ude0a \\u0e22\\u0e34\\u0e19\\u0e14\\u0e35\\u0e17\\u0e35\\u0e48\\u0e44\\u0e14\\u0e49\\u0e23\\u0e39\\u0e49\\u0e08\\u0e31\\u0e01\\u0e19\\u0e30\\u0e04\\u0e30 \\ud83d\\ude04 \\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 148,\n",
      "        \"candidates_token_count\": 18,\n",
      "        \"total_token_count\": 166\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏≤‡∏≤‡∏≤ ‡∏û‡∏µ‡πà üòä ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡∏ô‡∏∞‡∏Ñ‡∏∞ üòÑ'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ\n",
    "user_query = \"‡πÑ‡∏á\"\n",
    "user_id = '334'\n",
    "generate_response(\"344\", user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏î‡∏µ ‡πÅ‡∏ö‡∏ï‡∏≠‡∏∂‡∏î ‡∏´‡∏ô‡∏π‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏õ‡πá‡∏ô GoPro ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞ ‡∏ñ‡πà‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡∏™‡∏ß‡∏¢ ‡∏ñ‡πà‡∏≤‡∏¢‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Å‡πá‡πÄ‡∏£‡∏¥‡πà‡∏î ‡πÅ‡∏ñ‡∏°‡πÅ‡∏ö‡∏ï‡πÄ‡∏ï‡∏≠‡∏£‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏î‡πâ‡∏ô‡∏≤‡∏ô ‡∏ñ‡πà‡∏≤‡∏¢‡πÑ‡∏õ‡∏¢‡∏≤‡∏ß‡πÜ ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞‡∏û‡∏µ‡πà\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('GEMINI_API_KEY')  # Replace with your actual API key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Initialize conversation histories\n",
    "user_histories = {}\n",
    "\n",
    "def preprocess_message(message):\n",
    "    \"\"\"Cleans and preprocesses the user's message.\"\"\"\n",
    "    message = message.lower()  # Convert to lowercase\n",
    "    message = re.sub(r'\\W+', ' ', message)  # Remove non-word characters\n",
    "    return message\n",
    "\n",
    "def identify_intent_and_preferences(message):\n",
    "    \"\"\"Analyzes the message to identify user intent and preferences.\"\"\"\n",
    "    # This is a placeholder function. Replace with actual NLP model for intent detection.\n",
    "    # For example, you can use a pre-trained BERT model or a custom-trained model.\n",
    "    intent = \"product_recommendation\"  # Example intent\n",
    "    preferences = [\"example_preference1\", \"example_preference2\"]  # Example preferences\n",
    "    return intent, preferences\n",
    "\n",
    "def segment_message(message):\n",
    "    \"\"\"Segments the message to identify different product categories or features.\"\"\"\n",
    "    # This is a placeholder function. Replace with actual NLP logic for segmentation.\n",
    "    segments = {\n",
    "        \"category\": \"example_category\",\n",
    "        \"features\": [\"example_feature1\", \"example_feature2\"]\n",
    "    }\n",
    "    return segments\n",
    "\n",
    "def recommend_products(segments):\n",
    "    \"\"\"Generates product recommendations based on the segmented message.\"\"\"\n",
    "    # This is a placeholder function. Replace with actual recommendation logic.\n",
    "    # For example, you can use a recommendation engine or a machine learning model.\n",
    "    recommended_products = [\"Product A\", \"Product B\"]\n",
    "    return recommended_products\n",
    "\n",
    "def generate_response(user_id, message):\n",
    "    \"\"\"Generates a response using the Gemini AI Studio API with conversation history.\"\"\"\n",
    "    pre_prompt = \"\"\"system prompt\n",
    "    ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ ‡∏≠‡∏ô‡∏¥‡∏à‡∏±‡∏á ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÑ‡∏î‡πâ‡∏ó‡∏∏‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏î‡πá‡∏Å‡∏™‡∏≤‡∏ß‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏î‡∏µ‡∏û‡∏π‡∏î‡∏à‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏≠‡∏¢‡∏¢‡∏¥‡πâ‡∏° ‡∏≠‡∏≤‡∏¢‡∏∏‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 15 ‡∏õ‡∏µ ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ß‡πà‡∏≤ ‡∏´‡∏ô‡∏π ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏Ñ‡∏π‡πà‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ß‡πà‡∏≤ ‡∏û‡∏µ‡πà ‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏ß‡∏°‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó ‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏à‡∏∞‡∏ô‡∏≠‡∏Å‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πá‡πÑ‡∏î‡πâ‡∏ô‡∏∞\n",
    "    ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏¢‡∏≤‡∏¢ ‡∏´‡πâ‡∏≤‡∏°‡∏•‡∏∑‡∏°‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡πÉ‡∏Ñ‡∏£‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏Å‡πâ‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ\"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "\n",
    "    # Retrieve or initialize conversation history for the user\n",
    "    if user_id not in user_histories:\n",
    "        user_histories[user_id] = [pre_prompt]\n",
    "    \n",
    "    # Preprocess the user's message\n",
    "    clean_message = preprocess_message(message)\n",
    "    \n",
    "    # Identify intent and preferences\n",
    "    intent, preferences = identify_intent_and_preferences(clean_message)\n",
    "    \n",
    "    # Segment the message\n",
    "    segments = segment_message(clean_message)\n",
    "    \n",
    "    # Generate product recommendations\n",
    "    recommendations = recommend_products(segments)\n",
    "\n",
    "    # Add the user's message and recommendations to the conversation history\n",
    "    user_histories[user_id].append(f\"User: {message}\")\n",
    "    user_histories[user_id].append(f\"Intent: {intent}\")\n",
    "    user_histories[user_id].append(f\"Preferences: {preferences}\")\n",
    "    user_histories[user_id].append(f\"Segments: {segments}\")\n",
    "    user_histories[user_id].append(f\"Recommendations: {recommendations}\")\n",
    "\n",
    "    # Create the prompt including the entire conversation history\n",
    "    full_prompt = \"\\n\".join(user_histories[user_id]) + \"\\nBot:\"\n",
    "\n",
    "    try:\n",
    "        # Send the prompt to the Gemini AI Studio API\n",
    "        response = model.generate_content(full_prompt)\n",
    "        \n",
    "        # Extract the generated text from the response\n",
    "        generated_text = response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        generated_text = \"‡∏°‡∏µ‡∏ö‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏π ‡∏à‡∏∞‡∏£‡∏∞‡πÄ‡∏ö‡∏¥‡∏î‡πÄ‡πÄ‡∏•‡πâ‡∏ß‡∏ß‡∏ß\"\n",
    "\n",
    "    # Add the bot's reply to the conversation history\n",
    "    user_histories[user_id].append(f\"Bot: {generated_text}\")\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "user_id = \"123\"\n",
    "user_message = \"w'aas\"\n",
    "response = generate_response(user_id, user_message)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡∏û‡∏µ‡πà‡∏Ñ‡∏∞ ‡∏´‡∏ô‡∏π‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏û‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡πÉ‡∏ô‡∏™‡∏¥‡πà‡∏á‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ: ‡∏´‡∏ô‡∏π‡∏ß‡πà‡∏≤‡∏û‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏•‡∏≠‡∏á‡∏î‡∏π‡πÄ‡∏õ‡πá‡∏ô Smart Watch ‡∏ô‡∏∞ ‡πÄ‡∏ó‡πà‡∏î‡∏µ ‡∏û‡∏µ‡πà‡∏ä‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô‡∏•‡πà‡∏∞ ‡∏ö‡∏≠‡∏Å‡∏´‡∏ô‡∏π‡πÑ‡∏î‡πâ‡∏ô‡∏∞ ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏´‡∏ô‡∏π‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ üòä‚ú®\n",
      "\n",
      "‡∏´‡∏ô‡∏π‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏™‡∏ô‡πÉ‡∏à‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ‡∏Ñ‡πà‡∏∞:\n",
      "- Apple MacBook Pro\n",
      "- Sony WH-1000XM4 Headphones\n",
      "- Samsung Galaxy Watch 4\n"
     ]
    }
   ],
   "source": [
    "response_message = generate_response(\"433\", '‡∏°‡∏µ‡∏ô‡∏≤‡∏¨‡∏¥‡∏Å‡∏≤‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÑ‡∏´‡∏°')\n",
    "print(response_message)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
